**Description:** 
Use Apache Airflow to orchestrate a pipeline that moves data from S3 → Redshift, applying transformations with Python/PySpark.

**Tech Stack:** 
Airflow, Python, PySpark, Redshift, SQL.

**Key Steps:**
Airflow DAG scheduled daily.
Extract → load raw data from S3.
Transform with PySpark.
Insert into Redshift tables.
